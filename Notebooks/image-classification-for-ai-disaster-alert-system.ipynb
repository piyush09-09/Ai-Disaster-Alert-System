{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":269359,"sourceType":"datasetVersion","datasetId":111880},{"sourceId":805364,"sourceType":"datasetVersion","datasetId":421612},{"sourceId":12446385,"sourceType":"datasetVersion","datasetId":7851163},{"sourceId":12450638,"sourceType":"datasetVersion","datasetId":7854022},{"sourceId":12451742,"sourceType":"datasetVersion","datasetId":7854732},{"sourceId":12455297,"sourceType":"datasetVersion","datasetId":7856894},{"sourceId":12455849,"sourceType":"datasetVersion","datasetId":7857260},{"sourceId":12456682,"sourceType":"datasetVersion","datasetId":7857753},{"sourceId":12457597,"sourceType":"datasetVersion","datasetId":7858401}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models import resnet18\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:06:27.968971Z","iopub.execute_input":"2025-07-12T05:06:27.969610Z","iopub.status.idle":"2025-07-12T05:06:32.937368Z","shell.execute_reply.started":"2025-07-12T05:06:27.969579Z","shell.execute_reply":"2025-07-12T05:06:32.936785Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Intel Image Dataset Classification Section","metadata":{}},{"cell_type":"code","source":"data_dir= '/kaggle/input/intel-image-classification'\ntransform=transforms.Compose([\ntransforms.Resize((224,224)),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.485,0.456,0.406],\n                    std=[0.229,0.224,0.225])\n])\ntrain_dataset=ImageFolder(root=os.path.join(data_dir,'seg_train'),transform =transform)\ntest_dataset=ImageFolder(root=os.path.join(data_dir,'seg_test'),transform =transform)\n\ntrain_loader=DataLoader(train_dataset,batch_size=32,shuffle=True)\ntest_loader=DataLoader(test_dataset,batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:06:35.053306Z","iopub.execute_input":"2025-07-12T05:06:35.054014Z","iopub.status.idle":"2025-07-12T05:07:18.379699Z","shell.execute_reply.started":"2025-07-12T05:06:35.053969Z","shell.execute_reply":"2025-07-12T05:07:18.379154Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel=resnet18(pretrained=True)\nfor param in model.parameters():\n    param.required_grad=False\n\nmodel.fc=nn.Linear(model.fc.in_features,2)\nmodel=model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:07:18.380762Z","iopub.execute_input":"2025-07-12T05:07:18.380990Z","iopub.status.idle":"2025-07-12T05:07:19.193280Z","shell.execute_reply.started":"2025-07-12T05:07:18.380973Z","shell.execute_reply":"2025-07-12T05:07:19.192578Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 204MB/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"criterion =nn.CrossEntropyLoss()\noptimizer=torch.optim.Adam(model.fc.parameters(),lr=1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:07:23.047616Z","iopub.execute_input":"2025-07-12T05:07:23.048204Z","iopub.status.idle":"2025-07-12T05:07:23.052173Z","shell.execute_reply.started":"2025-07-12T05:07:23.048177Z","shell.execute_reply":"2025-07-12T05:07:23.051325Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nprint(\"‚úÖ CUDA available:\", torch.cuda.is_available())\nprint(\"üñ•Ô∏è Current device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:34:18.841618Z","iopub.execute_input":"2025-07-11T06:34:18.841859Z","iopub.status.idle":"2025-07-11T06:34:23.549875Z","shell.execute_reply.started":"2025-07-11T06:34:18.841836Z","shell.execute_reply":"2025-07-11T06:34:23.548934Z"}},"outputs":[{"name":"stdout","text":"‚úÖ CUDA available: True\nüñ•Ô∏è Current device: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def train_model(model, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total = 0\n        for batch_idx, (images, labels) in enumerate(train_loader):\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n                print(f\"  Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {100 * correct / total:.2f}%\")\ntrain_model(model, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:07:26.387540Z","iopub.execute_input":"2025-07-12T05:07:26.388049Z","iopub.status.idle":"2025-07-12T05:13:43.823903Z","shell.execute_reply.started":"2025-07-12T05:07:26.388015Z","shell.execute_reply":"2025-07-12T05:13:43.823126Z"}},"outputs":[{"name":"stdout","text":"  Batch 10/439 | Loss: 0.0057\n  Batch 20/439 | Loss: 0.0006\n  Batch 30/439 | Loss: 0.0003\n  Batch 40/439 | Loss: 0.0003\n  Batch 50/439 | Loss: 0.0002\n  Batch 60/439 | Loss: 0.0004\n  Batch 70/439 | Loss: 0.0002\n  Batch 80/439 | Loss: 0.0002\n  Batch 90/439 | Loss: 0.0002\n  Batch 100/439 | Loss: 0.0004\n  Batch 110/439 | Loss: 0.0003\n  Batch 120/439 | Loss: 0.0002\n  Batch 130/439 | Loss: 0.0002\n  Batch 140/439 | Loss: 0.0001\n  Batch 150/439 | Loss: 0.0002\n  Batch 160/439 | Loss: 0.0002\n  Batch 170/439 | Loss: 0.0002\n  Batch 180/439 | Loss: 0.0001\n  Batch 190/439 | Loss: 0.0001\n  Batch 200/439 | Loss: 0.0007\n  Batch 210/439 | Loss: 0.0001\n  Batch 220/439 | Loss: 0.0001\n  Batch 230/439 | Loss: 0.0001\n  Batch 240/439 | Loss: 0.0001\n  Batch 250/439 | Loss: 0.0001\n  Batch 260/439 | Loss: 0.0001\n  Batch 270/439 | Loss: 0.0001\n  Batch 280/439 | Loss: 0.0002\n  Batch 290/439 | Loss: 0.0001\n  Batch 300/439 | Loss: 0.0001\n  Batch 310/439 | Loss: 0.0001\n  Batch 320/439 | Loss: 0.0002\n  Batch 330/439 | Loss: 0.0001\n  Batch 340/439 | Loss: 0.0002\n  Batch 350/439 | Loss: 0.0001\n  Batch 360/439 | Loss: 0.0001\n  Batch 370/439 | Loss: 0.0002\n  Batch 380/439 | Loss: 0.0001\n  Batch 390/439 | Loss: 0.0002\n  Batch 400/439 | Loss: 0.0001\n  Batch 410/439 | Loss: 0.0001\n  Batch 420/439 | Loss: 0.0001\n  Batch 430/439 | Loss: 0.0001\n  Batch 439/439 | Loss: 0.0001\nEpoch 1/5, Loss: 2.2218, Accuracy: 99.75%\n  Batch 10/439 | Loss: 0.0001\n  Batch 20/439 | Loss: 0.0004\n  Batch 30/439 | Loss: 0.0001\n  Batch 40/439 | Loss: 0.0001\n  Batch 50/439 | Loss: 0.0001\n  Batch 60/439 | Loss: 0.0001\n  Batch 70/439 | Loss: 0.0001\n  Batch 80/439 | Loss: 0.0001\n  Batch 90/439 | Loss: 0.0001\n  Batch 100/439 | Loss: 0.0001\n  Batch 110/439 | Loss: 0.0001\n  Batch 120/439 | Loss: 0.0001\n  Batch 130/439 | Loss: 0.0002\n  Batch 140/439 | Loss: 0.0001\n  Batch 150/439 | Loss: 0.0001\n  Batch 160/439 | Loss: 0.0001\n  Batch 170/439 | Loss: 0.0001\n  Batch 180/439 | Loss: 0.0001\n  Batch 190/439 | Loss: 0.0001\n  Batch 200/439 | Loss: 0.0000\n  Batch 210/439 | Loss: 0.0000\n  Batch 220/439 | Loss: 0.0001\n  Batch 230/439 | Loss: 0.0002\n  Batch 240/439 | Loss: 0.0000\n  Batch 250/439 | Loss: 0.0000\n  Batch 260/439 | Loss: 0.0000\n  Batch 270/439 | Loss: 0.0001\n  Batch 280/439 | Loss: 0.0000\n  Batch 290/439 | Loss: 0.0000\n  Batch 300/439 | Loss: 0.0001\n  Batch 310/439 | Loss: 0.0000\n  Batch 320/439 | Loss: 0.0000\n  Batch 330/439 | Loss: 0.0000\n  Batch 340/439 | Loss: 0.0001\n  Batch 350/439 | Loss: 0.0000\n  Batch 360/439 | Loss: 0.0000\n  Batch 370/439 | Loss: 0.0001\n  Batch 380/439 | Loss: 0.0000\n  Batch 390/439 | Loss: 0.0000\n  Batch 400/439 | Loss: 0.0000\n  Batch 410/439 | Loss: 0.0000\n  Batch 420/439 | Loss: 0.0001\n  Batch 430/439 | Loss: 0.0001\n  Batch 439/439 | Loss: 0.0000\nEpoch 2/5, Loss: 0.0338, Accuracy: 100.00%\n  Batch 10/439 | Loss: 0.0000\n  Batch 20/439 | Loss: 0.0001\n  Batch 30/439 | Loss: 0.0000\n  Batch 40/439 | Loss: 0.0003\n  Batch 50/439 | Loss: 0.0001\n  Batch 60/439 | Loss: 0.0000\n  Batch 70/439 | Loss: 0.0000\n  Batch 80/439 | Loss: 0.0000\n  Batch 90/439 | Loss: 0.0000\n  Batch 100/439 | Loss: 0.0000\n  Batch 110/439 | Loss: 0.0000\n  Batch 120/439 | Loss: 0.0000\n  Batch 130/439 | Loss: 0.0000\n  Batch 140/439 | Loss: 0.0001\n  Batch 150/439 | Loss: 0.0000\n  Batch 160/439 | Loss: 0.0000\n  Batch 170/439 | Loss: 0.0000\n  Batch 180/439 | Loss: 0.0000\n  Batch 190/439 | Loss: 0.0000\n  Batch 200/439 | Loss: 0.0000\n  Batch 210/439 | Loss: 0.0000\n  Batch 220/439 | Loss: 0.0000\n  Batch 230/439 | Loss: 0.0000\n  Batch 240/439 | Loss: 0.0000\n  Batch 250/439 | Loss: 0.0001\n  Batch 260/439 | Loss: 0.0000\n  Batch 270/439 | Loss: 0.0000\n  Batch 280/439 | Loss: 0.0001\n  Batch 290/439 | Loss: 0.0000\n  Batch 300/439 | Loss: 0.0000\n  Batch 310/439 | Loss: 0.0000\n  Batch 320/439 | Loss: 0.0000\n  Batch 330/439 | Loss: 0.0000\n  Batch 340/439 | Loss: 0.0000\n  Batch 350/439 | Loss: 0.0000\n  Batch 360/439 | Loss: 0.0000\n  Batch 370/439 | Loss: 0.0000\n  Batch 380/439 | Loss: 0.0001\n  Batch 390/439 | Loss: 0.0000\n  Batch 400/439 | Loss: 0.0000\n  Batch 410/439 | Loss: 0.0000\n  Batch 420/439 | Loss: 0.0000\n  Batch 430/439 | Loss: 0.0001\n  Batch 439/439 | Loss: 0.0000\nEpoch 3/5, Loss: 0.0195, Accuracy: 100.00%\n  Batch 10/439 | Loss: 0.0000\n  Batch 20/439 | Loss: 0.0000\n  Batch 30/439 | Loss: 0.0000\n  Batch 40/439 | Loss: 0.0000\n  Batch 50/439 | Loss: 0.0000\n  Batch 60/439 | Loss: 0.0000\n  Batch 70/439 | Loss: 0.0000\n  Batch 80/439 | Loss: 0.0000\n  Batch 90/439 | Loss: 0.0000\n  Batch 100/439 | Loss: 0.0000\n  Batch 110/439 | Loss: 0.0000\n  Batch 120/439 | Loss: 0.0000\n  Batch 130/439 | Loss: 0.0000\n  Batch 140/439 | Loss: 0.0000\n  Batch 150/439 | Loss: 0.0000\n  Batch 160/439 | Loss: 0.0000\n  Batch 170/439 | Loss: 0.0000\n  Batch 180/439 | Loss: 0.0000\n  Batch 190/439 | Loss: 0.0001\n  Batch 200/439 | Loss: 0.0000\n  Batch 210/439 | Loss: 0.0000\n  Batch 220/439 | Loss: 0.0000\n  Batch 230/439 | Loss: 0.0000\n  Batch 240/439 | Loss: 0.0000\n  Batch 250/439 | Loss: 0.0000\n  Batch 260/439 | Loss: 0.0000\n  Batch 270/439 | Loss: 0.0000\n  Batch 280/439 | Loss: 0.0000\n  Batch 290/439 | Loss: 0.0000\n  Batch 300/439 | Loss: 0.0000\n  Batch 310/439 | Loss: 0.0000\n  Batch 320/439 | Loss: 0.0000\n  Batch 330/439 | Loss: 0.0000\n  Batch 340/439 | Loss: 0.0000\n  Batch 350/439 | Loss: 0.0000\n  Batch 360/439 | Loss: 0.0000\n  Batch 370/439 | Loss: 0.0000\n  Batch 380/439 | Loss: 0.0000\n  Batch 390/439 | Loss: 0.0000\n  Batch 400/439 | Loss: 0.0000\n  Batch 410/439 | Loss: 0.0000\n  Batch 420/439 | Loss: 0.0000\n  Batch 430/439 | Loss: 0.0000\n  Batch 439/439 | Loss: 0.0000\nEpoch 4/5, Loss: 0.0119, Accuracy: 100.00%\n  Batch 10/439 | Loss: 0.0000\n  Batch 20/439 | Loss: 0.0000\n  Batch 30/439 | Loss: 0.0000\n  Batch 40/439 | Loss: 0.0000\n  Batch 50/439 | Loss: 0.0000\n  Batch 60/439 | Loss: 0.0000\n  Batch 70/439 | Loss: 0.0000\n  Batch 80/439 | Loss: 0.0000\n  Batch 90/439 | Loss: 0.0000\n  Batch 100/439 | Loss: 0.0000\n  Batch 110/439 | Loss: 0.0000\n  Batch 120/439 | Loss: 0.0000\n  Batch 130/439 | Loss: 0.0000\n  Batch 140/439 | Loss: 0.0000\n  Batch 150/439 | Loss: 0.0000\n  Batch 160/439 | Loss: 0.0000\n  Batch 170/439 | Loss: 0.0000\n  Batch 180/439 | Loss: 0.0000\n  Batch 190/439 | Loss: 0.0000\n  Batch 200/439 | Loss: 0.0000\n  Batch 210/439 | Loss: 0.0000\n  Batch 220/439 | Loss: 0.0000\n  Batch 230/439 | Loss: 0.0000\n  Batch 240/439 | Loss: 0.0000\n  Batch 250/439 | Loss: 0.0000\n  Batch 260/439 | Loss: 0.0000\n  Batch 270/439 | Loss: 0.0000\n  Batch 280/439 | Loss: 0.0000\n  Batch 290/439 | Loss: 0.0000\n  Batch 300/439 | Loss: 0.0000\n  Batch 310/439 | Loss: 0.0000\n  Batch 320/439 | Loss: 0.0000\n  Batch 330/439 | Loss: 0.0000\n  Batch 340/439 | Loss: 0.0000\n  Batch 350/439 | Loss: 0.0000\n  Batch 360/439 | Loss: 0.0000\n  Batch 370/439 | Loss: 0.0000\n  Batch 380/439 | Loss: 0.0000\n  Batch 390/439 | Loss: 0.0000\n  Batch 400/439 | Loss: 0.0000\n  Batch 410/439 | Loss: 0.0000\n  Batch 420/439 | Loss: 0.0000\n  Batch 430/439 | Loss: 0.0000\n  Batch 439/439 | Loss: 0.0000\nEpoch 5/5, Loss: 0.0076, Accuracy: 100.00%\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def evaluate(model):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nevaluate(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:13:56.209233Z","iopub.execute_input":"2025-07-12T05:13:56.209489Z","iopub.status.idle":"2025-07-12T05:14:23.711190Z","shell.execute_reply.started":"2025-07-12T05:13:56.209471Z","shell.execute_reply":"2025-07-12T05:14:23.710380Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 100.00%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model_save_path = \"/kaggle/working/resnet_disaster_clean.pt\"\ntorch.save(model.state_dict(), model_save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:15:17.789084Z","iopub.execute_input":"2025-07-12T05:15:17.789873Z","iopub.status.idle":"2025-07-12T05:15:17.904293Z","shell.execute_reply.started":"2025-07-12T05:15:17.789849Z","shell.execute_reply":"2025-07-12T05:15:17.903750Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\nprint(os.path.exists(\"/kaggle/working/resnet_disaster_clean.pt\"))  # Should print True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:16:33.227679Z","iopub.execute_input":"2025-07-12T05:16:33.228085Z","iopub.status.idle":"2025-07-12T05:16:33.232625Z","shell.execute_reply.started":"2025-07-12T05:16:33.228050Z","shell.execute_reply":"2025-07-12T05:16:33.231857Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Xview2 Model","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\n\n# Path to the folder containing JSON files\njson_dir = \"/kaggle/input/labels/labels\"\n\ndata = []\n\n# Loop through all JSON files\nfor filename in os.listdir(json_dir):\n    if filename.endswith(\".json\"):\n        filepath = os.path.join(json_dir, filename)\n\n        with open(filepath, 'r') as f:\n            try:\n                label_data = json.load(f)\n                \n                # You can customize this extraction as needed\n                image_id = filename.replace('_post_disaster.json', '')\n                features = label_data.get(\"features\", {}).get(\"xy\", [])\n                label_type = label_data.get(\"metadata\", {}).get(\"disaster_type\", \"unknown\")\n\n                data.append({\n                    \"image_id\": image_id,\n                    \"num_features\": len(features),\n                    \"disaster_type\": label_type\n                })\n            except Exception as e:\n                print(f\"Error processing {filename}: {e}\")\n\n# Create a DataFrame and save as CSV\ndf = pd.DataFrame(data)\ndf.to_csv(\"/kaggle/working/train_labels.csv\", index=False)\n\nprint(\"‚úÖ CSV saved to /kaggle/working/train_labels.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:18:11.155843Z","iopub.execute_input":"2025-07-13T14:18:11.156109Z","iopub.status.idle":"2025-07-13T14:18:30.841739Z","shell.execute_reply.started":"2025-07-13T14:18:11.156092Z","shell.execute_reply":"2025-07-13T14:18:30.841091Z"}},"outputs":[{"name":"stdout","text":"‚úÖ CSV saved to /kaggle/working/train_labels.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:54:48.693968Z","iopub.execute_input":"2025-07-12T04:54:48.694215Z","iopub.status.idle":"2025-07-12T04:54:52.770042Z","shell.execute_reply.started":"2025-07-12T04:54:48.694190Z","shell.execute_reply":"2025-07-12T04:54:52.769000Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport tarfile\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset,DataLoader\nfrom PIL import Image\nimport pandas as pd\nimport glob\nimport shutil\n\n# setting up device\ndevice= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Loading pretrained model\nfrom torchvision.models import resnet18\nmodel =resnet18(weights=None)\nmodel.fc= nn.Linear(model.fc.in_features,2)\nmodel.load_state_dict(torch.load(\"/kaggle/working/resnet_disaster_clean.pt\",map_location=device))\nmodel = model.to(device)\n\n# Training hyperparameters\ncriterion =nn.CrossEntropyLoss()\noptimizer=torch.optim.Adam(model.parameters(),lr=1e-4)\nEPOCHS=5\nBATCH_SIZE=16\n\ntransform=transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor()\n])\n\n# Label mapping and loading the label CSV\nimport pandas as pd\n\n# Load your JSON-to-CSV converted file\nlabel_df = pd.read_csv(\"/kaggle/working/train_labels.csv\")\n\n# Convert to proper format for classification\n# image_id becomes image_name with \"_post_disaster\" suffix\nlabel_df[\"image_name\"] = label_df[\"image_id\"] + \"_post_disaster\"\n\n# Define label based on `num_features` as a proxy for damage level\n# This is arbitrary and can be changed if needed\ndef map_damage(num_features):\n    if num_features == 0:\n        return 0  # no-damage\n    elif num_features <= 2:\n        return 1  # minor\n    elif num_features <= 5:\n        return 2  # major\n    else:\n        return 3  # destroyed\n\nlabel_df[\"label\"] = label_df[\"num_features\"].apply(map_damage)\n\n# Final format required by dataset class\nlabel_df = label_df[[\"image_name\", \"label\"]]\nlabel_dict = dict(zip(label_df.image_name, label_df.label))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:29:43.564484Z","iopub.execute_input":"2025-07-12T16:29:43.564761Z","iopub.status.idle":"2025-07-12T16:29:43.831026Z","shell.execute_reply.started":"2025-07-12T16:29:43.564740Z","shell.execute_reply":"2025-07-12T16:29:43.830450Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Preparing a Custom dataset for 50 pairs\nclass XView2Dataset(Dataset):\n    def __init__(self, image_paths, transform=None):\n        self.image_paths = image_paths\n        self.transform = transform\n        self.labels = [label_dict.get(os.path.basename(p).replace(\".tif\", \"\"), 0) for p in image_paths]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:31:54.093154Z","iopub.execute_input":"2025-07-12T16:31:54.093875Z","iopub.status.idle":"2025-07-12T16:31:54.098822Z","shell.execute_reply.started":"2025-07-12T16:31:54.093850Z","shell.execute_reply":"2025-07-12T16:31:54.098024Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Training Function\ndef train_model(model, dataloader, optimizer, criterion, epochs=1):\n    model.train()\n    for epoch in range(epochs):\n        total_loss, correct, total = 0, 0, 0\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f} - Acc: {100 * correct / total:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:31:57.721883Z","iopub.execute_input":"2025-07-12T16:31:57.722548Z","iopub.status.idle":"2025-07-12T16:31:57.727504Z","shell.execute_reply.started":"2025-07-12T16:31:57.722524Z","shell.execute_reply":"2025-07-12T16:31:57.726857Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Making a loop to process .tgz files parts in chunks\nimport tarfile\nimport io\nfrom PIL import Image\n\ndef process_tgz_in_chunks(tgz_path, extract_dir, chunk_size=50):\n    with tarfile.open(tgz_path, \"r:gz\") as tar:\n        # Filter for post-disaster images\n        post_members = [m for m in tar.getmembers() if \"_post_disaster.tif\" in m.name and m.isfile()]\n        print(f\"Total post-disaster images in archive: {len(post_members)}\")\n\n        for i in range(0, len(post_members), chunk_size):\n            chunk_members = post_members[i:i+chunk_size]\n            chunk_paths = []\n\n            print(f\"\\nExtracting chunk {i}-{i+len(chunk_members)-1}...\")\n\n            for member in chunk_members:\n                member_path = os.path.join(extract_dir, os.path.basename(member.name))\n                chunk_paths.append(member_path)\n\n                # Extract image to memory then save to disk\n                f = tar.extractfile(member)\n                if f is not None:\n                    image = Image.open(io.BytesIO(f.read()))\n                    image.save(member_path)\n\n            # Train on this chunk\n            dataset = XView2Dataset(chunk_paths, transform=transform)\n            dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n            print(f\"Training on {len(chunk_paths)} images...\")\n            train_model(model, dataloader, optimizer, criterion, epochs=EPOCHS)\n\n            # Delete extracted images to free space\n            for path in chunk_paths:\n                if os.path.exists(path):\n                    os.remove(path)\n\n    print(\"‚úÖ Finished processing archive chunk-by-chunk.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:39:01.834024Z","iopub.execute_input":"2025-07-12T16:39:01.834300Z","iopub.status.idle":"2025-07-12T16:39:01.841614Z","shell.execute_reply.started":"2025-07-12T16:39:01.834277Z","shell.execute_reply":"2025-07-12T16:39:01.840893Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"file_ids = {\n    \"aa\": \"12t_PXcgNjAuxHDPi_pVtDT4YUQe3Piy0\",\n    \"ab\": \"1_N24_6Gj2zBfxTOPaOYUJz6zcucK4kzo\",\n    \"ac\": \"1-xluFLB5YgbsZw8W4JDtNXNDukInWn5m\",\n    \"ad\": \"1VKV9g3U8pS6dLByi3dKTj-Orc388v3mI\",\n    \"ae\": \"1ylemTUM1_r6GM11sYXiS7ivawtLtdgxo\",\n    \"af\": \"1QZlwXVKw8BUAnT1yL2W1Lu-Wynl3n0uQ\",\n}\n#https://drive.google.com/file/d/12t_PXcgNjAuxHDPi_pVtDT4YUQe3Piy0/view?usp=drive_link\n#https://drive.google.com/file/d/1_N24_6Gj2zBfxTOPaOYUJz6zcucK4kzo/view?usp=sharing\n#https://drive.google.com/file/d/1-xluFLB5YgbsZw8W4JDtNXNDukInWn5m/view?usp=sharing\n#https://drive.google.com/file/d/1VKV9g3U8pS6dLByi3dKTj-Orc388v3mI/view?usp=sharing\n#https://drive.google.com/file/d/1ylemTUM1_r6GM11sYXiS7ivawtLtdgxo/view?usp=sharing\n#https://drive.google.com/file/d/1QZlwXVKw8BUAnT1yL2W1Lu-Wynl3n0uQ/view?usp=sharing\n\nfor part, file_id in file_ids.items():\n    tgz_name = f\"xview2_geotiff_part_{part}.tgz\"\n    extract_path = \"/kaggle/working/tmp_extract\"\n\n    if os.path.exists(extract_path):\n        shutil.rmtree(extract_path)\n    os.makedirs(extract_path, exist_ok=True)\n\n    print(f\"\\nüì• Downloading part {part}...\")\n    os.system(f\"gdown https://drive.google.com/uc?id={file_id} -O {tgz_name}\")\n\n    process_tgz_in_chunks(tgz_name, extract_path, chunk_size=50)\n\n    checkpoint_path = f\"/kaggle/working/resnet_checkpoint_part_{part}.pt\"\n    torch.save(model.state_dict(), checkpoint_path)\n    print(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n\n# Final model save\ntorch.save(model.state_dict(), \"/kaggle/working/resnet_disaster_final.pt\")\nprint(\"‚úÖ Final model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:39:04.902221Z","iopub.execute_input":"2025-07-12T16:39:04.903002Z","iopub.status.idle":"2025-07-12T16:42:28.528377Z","shell.execute_reply.started":"2025-07-12T16:39:04.902972Z","shell.execute_reply":"2025-07-12T16:42:28.527447Z"}},"outputs":[{"name":"stdout","text":"\nüì• Downloading part aa...\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=12t_PXcgNjAuxHDPi_pVtDT4YUQe3Piy0\nFrom (redirected): https://drive.google.com/uc?id=12t_PXcgNjAuxHDPi_pVtDT4YUQe3Piy0&confirm=t&uuid=5a4e420e-f9f3-448c-986a-1c97c733ffd6\nTo: /kaggle/working/xview2_geotiff_part_aa.tgz\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.5G/10.5G [00:40<00:00, 259MB/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3088955063.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"gdown https://drive.google.com/uc?id={file_id} -O {tgz_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mprocess_tgz_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgz_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/kaggle/working/resnet_checkpoint_part_{part}.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1051645056.py\u001b[0m in \u001b[0;36mprocess_tgz_in_chunks\u001b[0;34m(tgz_path, extract_dir, chunk_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r:gz\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Filter for post-disaster images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpost_members\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmembers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"_post_disaster.tif\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total post-disaster images in archive: {len(post_members)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36mgetmembers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2039\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# if we want to obtain a list of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# all members, we first have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                 \u001b[0;31m# scan the whole archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2815\u001b[0m         \"\"\"\n\u001b[1;32m   2816\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2817\u001b[0;31m             \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2819\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2720\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mseek\u001b[0;34m(self, offset, whence)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/_compression.py\u001b[0m in \u001b[0;36mseek\u001b[0;34m(self, offset, whence)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Read and discard data until we reach the desired position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 raise EOFError(\"Compressed file ended before the \"\n\u001b[0m\u001b[1;32m    519\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"],"ename":"EOFError","evalue":"Compressed file ended before the end-of-stream marker was reached","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"import os\ncorrupted_file = \"/kaggle/working/xview2_geotiff_part_aa.tgz\"\nif os.path.exists(corrupted_file):\n    os.remove(corrupted_file)\n    print(\"üßπ Corrupted .tgz deleted. Ready to re-download.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:44:16.200175Z","iopub.execute_input":"2025-07-12T16:44:16.200535Z","iopub.status.idle":"2025-07-12T16:44:16.204547Z","shell.execute_reply.started":"2025-07-12T16:44:16.200511Z","shell.execute_reply":"2025-07-12T16:44:16.203855Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"!gdown --fuzzy --id 12t_PXcgNjAuxHDPi_pVtDT4YUQe3Piy0 -O /kaggle/working/xview2_geotiff_part_aa.tgz\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:44:40.800776Z","iopub.execute_input":"2025-07-12T16:44:40.801491Z","iopub.status.idle":"2025-07-12T16:44:43.354115Z","shell.execute_reply.started":"2025-07-12T16:44:40.801467Z","shell.execute_reply":"2025-07-12T16:44:43.353391Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nFailed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=12t_PXcgNjAuxHDPi_pVtDT4YUQe3Piy0\n\nbut Gdown can't. Please check connections and permissions.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import os\nimport shutil\nimport glob\n\n# Paths to preserve\nkeep_files = {\n    \"/kaggle/working/train_labels.csv\",\n    \"/kaggle/working/resnet_disaster_clean.pt\"\n}\n\n# Delete all files and folders in /kaggle/working except the important ones\nfor item in os.listdir(\"/kaggle/working\"):\n    item_path = os.path.join(\"/kaggle/working\", item)\n    if item_path not in keep_files:\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.remove(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n            print(f\"üßπ Deleted: {item_path}\")\n        except Exception as e:\n            print(f\"‚ùå Could not delete {item_path}: {e}\")\n\nprint(\"‚úÖ Cleanup complete. Only train_labels.csv and resnet_disaster_clean.pt are kept.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:43:55.279136Z","iopub.execute_input":"2025-07-12T16:43:55.279439Z","iopub.status.idle":"2025-07-12T16:43:57.328449Z","shell.execute_reply.started":"2025-07-12T16:43:55.279417Z","shell.execute_reply":"2025-07-12T16:43:57.327663Z"}},"outputs":[{"name":"stdout","text":"üßπ Deleted: /kaggle/working/tmp_extract\nüßπ Deleted: /kaggle/working/xview2_geotiff_part_aa.tgz\n‚úÖ Cleanup complete. Only train_labels.csv and resnet_disaster_clean.pt are kept.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Adjust the input path to match your dataset's mount path\nshutil.copy(\"/kaggle/input/resnet-disaster-clean/resnet_disaster_clean.pt\", \"/kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:14:10.076954Z","iopub.execute_input":"2025-07-13T14:14:10.077679Z","iopub.status.idle":"2025-07-13T14:14:10.735430Z","shell.execute_reply.started":"2025-07-13T14:14:10.077649Z","shell.execute_reply":"2025-07-13T14:14:10.734837Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/resnet_disaster_clean.pt'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import shutil\n\n# Adjust the input path to match your dataset's mount path\nshutil.copy(\"/kaggle/input/labels/labels.zip\", \"/kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T15:55:52.096554Z","iopub.execute_input":"2025-07-12T15:55:52.097225Z","iopub.status.idle":"2025-07-12T15:55:52.107778Z","shell.execute_reply.started":"2025-07-12T15:55:52.097203Z","shell.execute_reply":"2025-07-12T15:55:52.106868Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1150787300.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Adjust the input path to match your dataset's mount path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/labels/labels.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/kaggle/working/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/labels/labels.zip'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/labels/labels.zip'","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_ids = {\n    \"aa\": \"12t_PXcgNjAuxHDPi_pVtDT4YUQe3Piy0\",\n    \"ab\": \"1_N24_6Gj2zBfxTOPaOYUJz6zcucK4kzo\",\n    \"ac\": \"1-xluFLB5YgbsZw8W4JDtNXNDukInWn5m\",\n    \"ad\": \"1VKV9g3U8pS6dLByi3dKTj-Orc388v3mI\",\n    \"ae\": \"1ylemTUM1_r6GM11sYXiS7ivawtLtdgxo\",\n    \"af\": \"1QZlwXVKw8BUAnT1yL2W1Lu-Wynl3n0uQ\",\n}\n#https://drive.google.com/file/d/12t_PXcgNjAuxHDPi_pVtDT4YUQe3Piy0/view?usp=drive_link\n#https://drive.google.com/file/d/1_N24_6Gj2zBfxTOPaOYUJz6zcucK4kzo/view?usp=sharing\n#https://drive.google.com/file/d/1-xluFLB5YgbsZw8W4JDtNXNDukInWn5m/view?usp=sharing\n#https://drive.google.com/file/d/1VKV9g3U8pS6dLByi3dKTj-Orc388v3mI/view?usp=sharing\n#https://drive.google.com/file/d/1ylemTUM1_r6GM11sYXiS7ivawtLtdgxo/view?usp=sharing\n#https://drive.google.com/file/d/1QZlwXVKw8BUAnT1yL2W1Lu-Wynl3n0uQ/view?usp=sharing\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel =resnet18(pretrained =False)\nmodel.fc=nn.Linear(model.fc.in_features,2)\nmodel.load_state_dict(torch.load(\"/kaggle/working/resnet_disaster_clean.pt\",map_location=device))\nmodel=model.to(device)\ncriterion=nn.CrossEntropyLoss()\noptimizer=torch.optim.Adam(model.fc.parameters(),lr=1e-4)\n\nfrom torch.utils.data import DataLoader\n\ndef train_model(model,train_loader,epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        total_loss=0\n        correct=0\n        total=0\n        for batch_idx,(images,labels) in emumerate(train_loader):\n            images,labels = images.to(device),labels.to(device)\n            optimizer.zero_grad()\n            outputs=model(images)\n            loss=criterion(ouputs,labels)\n            loss.backward()\n            optimizer.step()\n            total_loss +=loss.item()\n            _,predicted=torch.max(outputs,1)\n            correct += (predicted == lables).sum().item()\n            total += labels.size(0)\n    \n            if(batch_idx +1)%10==0 or (batch_idx +1)==len(train_loader):\n                print(f\"  Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n\n\nfrom torchvision import datasets, transforms\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor()\n])\n\n\nimport os\nfor part,file_id in file_ids.items():\n    tgz_filename= f\"xview2_geotiff_part_{part}.tgz\"\n    extract_path= \"/kaggle/working/xview2_data\"\n\n    !rm -rf {extract_path}\n    os.makedirs(extract_path,exist_ok =True)\n    print(f\"\\n Downloading {tgz_filename}..\")\n    !gdown --id {file_id} --output {tgz_filename}\n\n    print(f\" Extracting {tgz_filename}..\")\n    !tar -xvzf {tgz_filename} -C {extract_path}\n\n    train_dataset=datasets.Imagefolder(root=extract_path,transform =transform)\n    train_loader=DataLoader(train_dataset,batch_size=32,shuffle= True)\n\n    print(f\"Training on part {part}..\")\n    train_model(model,train_loader,epochs=5)\n\n    checkpoint_path=f\"/kaggle/working/resnet_finetuned_part_{part}.pth\"\n    torch.save(model.state_dict(),checkpoint_path)\n    print(f\"checkpoint saved : {checkpoint_path}\")\n\n\ntorch.save(model.state_dict(),\"/kaggle/working/final_resnet_disaster.pt\")\nprint(\"Final model Saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tarfile\nimport shutil\nfrom glob import glob\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom tqdm import tqdm\nimport pandas as pd\n\n# ========== CONFIG ========== #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlabels_csv_path = \"/kaggle/working/train_labels.csv\"  # Update if your file path is different\ntgz_paths = [\n    \"/kaggle/input/xview2-geotiff-part-aa/xview2_geotiff_part_aa.tgz\",\n    \"/kaggle/input/xview2-geotiff-part-ab1/xview2_geotiff_part_ab.tgz\",\n    \"/kaggle/input/xview2-geotiff-part-ac/xview2_geotiff_part_ac.tgz\",\n    \"/kaggle/input/xview2-geotiff-part-ad/xview2_geotiff_part_ad.tgz\",\n    \"/kaggle/input/xview2-geotiff-part-ae/xview2_geotiff_part_ae.tgz\",\n    \"/kaggle/input/xview2-geotiff-part-af/xview2_geotiff_part_af.tgz\"\n]\nextract_dir = \"/kaggle/temp\"\nchunk_size = 50\n\n# ========== MODEL ========== #\nclass SatelliteDataset(Dataset):\n    def __init__(self, image_paths, labels_df, transform=None):\n        self.image_paths = image_paths\n        self.labels_df = labels_df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        path = self.image_paths[idx]\n        image = Image.open(path).convert('RGB')\n        image_id = os.path.basename(path).replace(\"_post_disaster.tif\", \"\")\n        label = self.labels_df.loc[self.labels_df[\"image_id\"] == image_id, \"label\"].values[0]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\ndef get_model(num_classes):\n    model = models.resnet18(pretrained=True)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model.to(device)\n\n# ========== TRAIN FUNCTION ========== #\ndef train_one_epoch(model, dataloader, criterion, optimizer):\n    model.train()\n    total_loss, correct = 0, 0\n    for images, labels in tqdm(dataloader, leave=False):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        correct += (outputs.argmax(1) == labels).sum().item()\n    return total_loss / len(dataloader), correct / len(dataloader.dataset)\n\n# ========== PROCESS TGZ ========== #\ndef process_tgz_in_chunks(tgz_path, extract_base, chunk_size):\n    with tarfile.open(tgz_path, \"r:gz\") as tar:\n        post_members = [m for m in tar.getmembers() if \"_post_disaster.tif\" in m.name and m.isfile()]\n        print(f\"Found {len(post_members)} post-disaster images.\")\n        for i in range(0, len(post_members), chunk_size):\n            chunk = post_members[i:i+chunk_size]\n            chunk_dir = os.path.join(extract_base, f\"chunk_{i}\")\n            os.makedirs(chunk_dir, exist_ok=True)\n            for member in chunk:\n                member.name = os.path.basename(member.name)  # Avoid nested folders\n                tar.extract(member, path=chunk_dir)\n            yield chunk_dir\n            shutil.rmtree(chunk_dir)  # Clean up\n\n# ========== LABEL MAPPING ========== #\nlabels_df = pd.read_csv(labels_csv_path)\n# Define your own label logic if needed\ndef map_features_to_label(num_features):\n    if num_features == 0:\n        return 0\n    elif num_features <= 5:\n        return 1\n    elif num_features <= 15:\n        return 2\n    else:\n        return 3\nlabels_df[\"label\"] = labels_df[\"num_features\"].apply(map_features_to_label)\n\n# ========== TRANSFORM ========== #\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\n# ========== LOOP THROUGH TGZ FILES ========== #\nmodel = get_model(num_classes=4)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\nfor part_idx, tgz_path in enumerate(tgz_paths):\n    print(f\"\\n==> Processing {os.path.basename(tgz_path)}\")\n\n    for chunk_dir in process_tgz_in_chunks(tgz_path, extract_dir, chunk_size):\n        image_paths = sorted(glob(os.path.join(chunk_dir, \"*.tif\")))\n        dataset = SatelliteDataset(image_paths, labels_df, transform)\n        dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n\n        loss, acc = train_one_epoch(model, dataloader, criterion, optimizer)\n        print(f\"Chunk: Loss = {loss:.4f}, Accuracy = {acc:.4f}\")\n\n    checkpoint_path = f\"/kaggle/working/resnet_checkpoint_part_{part_idx+1}.pt\"\n    torch.save(model.state_dict(), checkpoint_path)\n    print(f\"‚úÖ Saved checkpoint: {checkpoint_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:26:09.454497Z","iopub.execute_input":"2025-07-13T14:26:09.454774Z","iopub.status.idle":"2025-07-13T14:29:39.081083Z","shell.execute_reply.started":"2025-07-13T14:26:09.454754Z","shell.execute_reply":"2025-07-13T14:29:39.079905Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 186MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\n==> Processing xview2_geotiff_part_aa.tgz\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1315913071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n==> Processing {os.path.basename(tgz_path)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocess_tgz_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*.tif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSatelliteDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1315913071.py\u001b[0m in \u001b[0;36mprocess_tgz_in_chunks\u001b[0;34m(tgz_path, extract_base, chunk_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_tgz_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r:gz\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mpost_members\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmembers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"_post_disaster.tif\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {len(post_members)} post-disaster images.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36mgetmembers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2039\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# if we want to obtain a list of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# all members, we first have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                 \u001b[0;31m# scan the whole archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2815\u001b[0m         \"\"\"\n\u001b[1;32m   2816\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2817\u001b[0;31m             \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2819\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2720\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mseek\u001b[0;34m(self, offset, whence)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/_compression.py\u001b[0m in \u001b[0;36mseek\u001b[0;34m(self, offset, whence)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Read and discard data until we reach the desired position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 raise EOFError(\"Compressed file ended before the \"\n\u001b[0m\u001b[1;32m    519\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"],"ename":"EOFError","evalue":"Compressed file ended before the end-of-stream marker was reached","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"import tarfile\nfrom pathlib import Path\n\n# List of your dataset files on Kaggle\ndataset_paths = [\n    \"/kaggle/input/xview2-geotiff-part-aa/xview2_geotiff_part_aa.tgz\",\n    \"/kaggle/input/xview2-geotiff-part-ab/xview2_geotiff_part_ab.tgz\",\n    \"/kaggle/input/xview2-geotiff-part-ac/xview2_geotiff_part_ac.tgz\",\n    \"/kaggle/input/xview2-geotiff-part-ad/xview2_geotiff_part_ad.tgz\",\n    \"/kaggle/input/xview2-geotiff-part-ae/xview2_geotiff_part_ae.tgz\",\n    \"/kaggle/input/xview2-geotiff-part-af/xview2_geotiff_part_af.tgz\"\n]\n\nfor path in dataset_paths:\n    print(f\"üîç Testing {path}...\")\n    try:\n        with tarfile.open(path, \"r:gz\") as tar:\n            tar.getmembers()  # just list contents\n        print(\"‚úÖ File is OK.\\n\")\n    except Exception as e:\n        print(f\"‚ùå File is BROKEN: {e}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:31:04.455486Z","iopub.execute_input":"2025-07-13T14:31:04.455756Z","iopub.status.idle":"2025-07-13T14:33:40.696006Z","shell.execute_reply.started":"2025-07-13T14:31:04.455736Z","shell.execute_reply":"2025-07-13T14:33:40.695148Z"}},"outputs":[{"name":"stdout","text":"üîç Testing /kaggle/input/xview2-geotiff-part-aa/xview2_geotiff_part_aa.tgz...\n‚ùå File is BROKEN: Compressed file ended before the end-of-stream marker was reached\n\nüîç Testing /kaggle/input/xview2-geotiff-part-ab/xview2_geotiff_part_ab.tgz...\n‚ùå File is BROKEN: [Errno 2] No such file or directory: '/kaggle/input/xview2-geotiff-part-ab/xview2_geotiff_part_ab.tgz'\n\nüîç Testing /kaggle/input/xview2-geotiff-part-ac/xview2_geotiff_part_ac.tgz...\n‚ùå File is BROKEN: not a gzip file\n\nüîç Testing /kaggle/input/xview2-geotiff-part-ad/xview2_geotiff_part_ad.tgz...\n‚ùå File is BROKEN: not a gzip file\n\nüîç Testing /kaggle/input/xview2-geotiff-part-ae/xview2_geotiff_part_ae.tgz...\n‚ùå File is BROKEN: not a gzip file\n\nüîç Testing /kaggle/input/xview2-geotiff-part-af/xview2_geotiff_part_af.tgz...\n‚ùå File is BROKEN: [Errno 2] No such file or directory: '/kaggle/input/xview2-geotiff-part-af/xview2_geotiff_part_af.tgz'\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load your CSV\nlabels_df = pd.read_csv(\"/kaggle/working/train_labels.csv\")\n\n# Display first few rows\nprint(\"üìÑ Preview of labels_train.csv:\")\nprint(labels_df.head())\n\n# Display column names\nprint(\"\\nüß© Columns in CSV:\")\nprint(labels_df.columns.tolist())\n\n# Check for missing values\nprint(\"\\n‚ùó Missing values check:\")\nprint(labels_df.isnull().sum())\n\n# Check data types\nprint(\"\\nüîç Data types:\")\nprint(labels_df.dtypes)\n\n# Count rows\nprint(f\"\\nüî¢ Total rows: {len(labels_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:19:55.113905Z","iopub.execute_input":"2025-07-13T14:19:55.114172Z","iopub.status.idle":"2025-07-13T14:19:55.139744Z","shell.execute_reply.started":"2025-07-13T14:19:55.114153Z","shell.execute_reply":"2025-07-13T14:19:55.139169Z"}},"outputs":[{"name":"stdout","text":"üìÑ Preview of labels_train.csv:\n                                         image_id  num_features disaster_type\n0  santa-rosa-wildfire_00000138_pre_disaster.json           258          fire\n1     hurricane-harvey_00000041_pre_disaster.json             3      flooding\n2                      hurricane-matthew_00000295             6          wind\n3                             socal-fire_00000723             7          fire\n4    hurricane-michael_00000020_pre_disaster.json             2          wind\n\nüß© Columns in CSV:\n['image_id', 'num_features', 'disaster_type']\n\n‚ùó Missing values check:\nimage_id         0\nnum_features     0\ndisaster_type    0\ndtype: int64\n\nüîç Data types:\nimage_id         object\nnum_features      int64\ndisaster_type    object\ndtype: object\n\nüî¢ Total rows: 5598\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Apply this immediately after reading the CSV\nlabels_df[\"image_id\"] = labels_df[\"image_id\"].apply(\n    lambda x: x.replace(\"_pre_disaster.json\", \"_post_disaster.tif\")\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:21:06.438818Z","iopub.execute_input":"2025-07-13T14:21:06.439380Z","iopub.status.idle":"2025-07-13T14:21:06.446083Z","shell.execute_reply.started":"2025-07-13T14:21:06.439355Z","shell.execute_reply":"2025-07-13T14:21:06.445438Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(labels_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:21:20.535614Z","iopub.execute_input":"2025-07-13T14:21:20.536105Z","iopub.status.idle":"2025-07-13T14:21:20.541837Z","shell.execute_reply.started":"2025-07-13T14:21:20.536081Z","shell.execute_reply":"2025-07-13T14:21:20.541123Z"}},"outputs":[{"name":"stdout","text":"                                         image_id  num_features disaster_type\n0  santa-rosa-wildfire_00000138_post_disaster.tif           258          fire\n1     hurricane-harvey_00000041_post_disaster.tif             3      flooding\n2                      hurricane-matthew_00000295             6          wind\n3                             socal-fire_00000723             7          fire\n4    hurricane-michael_00000020_post_disaster.tif             2          wind\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}