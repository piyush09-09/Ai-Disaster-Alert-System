{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8820b7cc-ed43-484b-b23f-e1d14a1bfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0cac7-f3e7-4571-aa75-8f3876ef9296",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ca14d25-da7f-4c95-8627-dd753ec8cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "416f417c-45dd-4654-ae3c-f243c7439d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f2f65c8-5f23-4bd6-96d9-c7a0b472ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['id','keyword','location'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90b472f0-120a-44b5-a1ef-1313c81d540f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e7a435b-4f15-4c6a-bd9a-d1580e6e974d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3489cf2d-4690-40d3-b85c-78377313faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8e8fb7a-b4db-4009-8e13-f143f4e7352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'text':'tweets'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dac3ac0-407c-4e4a-bfb0-306786712a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7613/7613 [02:01<00:00, 62.75it/s]\n"
     ]
    }
   ],
   "source": [
    "def processed_tweets(tweet):\n",
    "    tweet=str(tweet).lower()\n",
    "    tweet=re.sub(r\"http\\S+|www|S+|https\\S+\",'',tweet,flags=re.MULTILINE)\n",
    "    tweet=re.sub(r\"@\\w+|#\\w+\",'',tweet)\n",
    "    tweet=re.sub(r\"[^\\w\\s]\",'',tweet)\n",
    "    tweet=re.sub(r\"\\d+\",'',tweet)\n",
    "    tokens=[token.lemma_ for token in nlp(tweet)]\n",
    "    tokens=[token for token in tokens if token not in stopwords.words('english')]\n",
    "    tokens=[token for token in tokens if len(token)>1]\n",
    "    processed_tweet=' '.join(tokens)\n",
    "    return processed_tweet\n",
    "    \n",
    "df['clean_tweets']=[processed_tweets(tw) for tw in tqdm(df['tweets'],position=0,leave=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1969125-d68c-4c78-8427-e70b9d1b6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='tweets', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79afc549-44e0-4426-8c02-8a6b9697d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/User/OneDrive/Desktop/Ai-Disaster-Alert-System/Data/Processed/cleaned_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa477cd-87d4-4505-ab9d-9adfd5843daf",
   "metadata": {},
   "source": [
    "# Performing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1252cd3-49f8-4cc7-8686-661e1e7efa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:/Users/User/OneDrive/Desktop/Ai-Disaster-Alert-System/Data/Processed/cleaned_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81863c98-e7a5-410f-9605-3c136d566c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pos_count=len(df[df['target']==1])\n",
    "#neg_count=len(df[df['target']==0])\n",
    "#fig=px.pie(values=[pos_count,neg_count],title='Distibution of target',names=['positive','negative'],opacity=.9)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d814626-37e3-402a-92bf-ed13f0d226ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>deed reason may allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>people receive evacuation order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>get send photo ruby smoke pour school</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                       clean_tweets\n",
       "0       1                      deed reason may allah forgive\n",
       "1       1              forest fire near la ronge sask canada\n",
       "2       1  resident ask shelter place notify officer evac...\n",
       "3       1         people receive evacuation order california\n",
       "4       1              get send photo ruby smoke pour school"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50a4b0ef-c068-4e5a-8c0c-251995132fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(20,8))\n",
    "#plt.subplot(1,2,1)\n",
    "#wordcloud1=WordCloud(width=1200,height=800,max_words=1000,contour_width=2,background_color='white',max_font_size=180,\n",
    "                     #colormap='viridis').generate(' '.join(df[df['target']==1]['clean_tweets'].dropna().astype(str)))\n",
    "#plt.imshow(wordcloud1,interpolation='bilinear')\n",
    "#plt.axis('off')\n",
    "#plt.title('positive')\n",
    "\n",
    "#plt.subplot(1,2,2)\n",
    "#wordcloud2=WordCloud(width=1200,height=800,max_words=1000,contour_width=2,background_color='black',max_font_size=180,\n",
    "                     #colormap='viridis').generate(' '.join(df[df['target']==0]['clean_tweets'].dropna().astype(str)))\n",
    "#plt.imshow(wordcloud2,interpolation='bilinear')\n",
    "#plt.axis('off')\n",
    "#plt.title('negative')\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafee736-7381-4088-b4a8-5c6b4204f882",
   "metadata": {},
   "source": [
    "# Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9113b61-5b06-4338-be6a-e7cb74adf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing the Data\n",
    "from datasets import Dataset\n",
    "x_train,x_test,y_train,y_test=train_test_split(df['clean_tweets'].tolist(),df['target'].tolist(),test_size=0.2,stratify=df['target'],random_state=42)\n",
    "# Sanitize inputs: ensure list of strings\n",
    "x_train = [str(text) for text in x_train]\n",
    "x_test = [str(text) for text in x_test]\n",
    "\n",
    "train_df = pd.DataFrame({'Clean_tweets': x_train, 'target': y_train})\n",
    "test_df = pd.DataFrame({'Clean_tweets': x_test, 'target': y_test})\n",
    "\n",
    "x_train_dataset=Dataset.from_pandas(train_df)\n",
    "x_test_dataset=Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c6e5a18-dbc5-4cbf-a41e-fa34e85fd8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a23eba89b9432a921af9bf885acbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94482d9b17b04841805f78e7236dd847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1523 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Tokenizing with Hugging face\n",
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def tokenize_function(dataset):\n",
    "    encoding = tokenizer(dataset['Clean_tweets'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    encoding[\"labels\"] = dataset[\"target\"]  # <-- Key Fix\n",
    "    return encoding\n",
    "x_train_dataset = x_train_dataset.map(tokenize_function, batched=True)\n",
    "x_test_dataset = x_test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58d5e438-b8f3-4119-a3c0-faf4418c505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting format to pytorch\n",
    "x_train_dataset.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "x_test_dataset.set_format(type='torch',columns=['input_ids','attention_mask','labels'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6919d86-59aa-4fcf-9055-b93f259ee6a7",
   "metadata": {},
   "source": [
    "# Building and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f176f2f-876f-47ac-8e55-dbf729b43e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 06:32:06.972000 10072 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1143' max='1143' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1143/1143 1:45:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.359700</td>\n",
       "      <td>0.455540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.422375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.230200</td>\n",
       "      <td>0.506053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training complete. Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "# Set safe output directory\n",
    "output_dir = r\"C:\\MLTraining\\bert_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create logging directory inside output_dir\n",
    "logging_dir = os.path.join(output_dir, \"logs\")\n",
    "os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "# Load pre-trained BERT\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\",  # Disable checkpoint saving\n",
    "    logging_dir=logging_dir,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainingArgs,\n",
    "    train_dataset=x_train_dataset,\n",
    "    eval_dataset=x_test_dataset,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(output_dir)\n",
    "print(\"‚úÖ Training complete. Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86d0dbee-a520-4cba-88d1-4c52100004ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 11:40:34.203000 20064 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(r\"C:\\MLTraining\\bert_results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b0191a4-f13a-406b-b8fe-c9c1e88be813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir =r\"C:\\Users\\User\\OneDrive\\Desktop\\Ai-Disaster-Alert-System\\Models\\MLTraining\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\",\n",
    "    logging_dir =r\"C:\\Users\\User\\OneDrive\\Desktop\\Ai-Disaster-Alert-System\\Models\\MLTraining\\logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainingArgs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7d56663-5fb9-4802-ae53-a545bc974e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       869\n",
      "           1       0.81      0.77      0.79       654\n",
      "\n",
      "    accuracy                           0.83      1523\n",
      "   macro avg       0.82      0.82      0.82      1523\n",
      "weighted avg       0.82      0.83      0.82      1523\n",
      "\n",
      "üìä Confusion Matrix:\n",
      "[[751 118]\n",
      " [148 506]]\n",
      "‚úÖ Accuracy: 82.53%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Step 1: Predict on x_test_dataset\n",
    "predictions = trainer.predict(x_test_dataset)\n",
    "\n",
    "# Step 2: Get predicted labels (argmax of logits)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Step 3: Get true labels\n",
    "y_true = np.array(x_test_dataset[\"target\"])  # or x_test_dataset[\"labels\"] depending on column name\n",
    "\n",
    "# Step 4: Evaluation\n",
    "print(\"üîç Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"üìä Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"‚úÖ Accuracy: {acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4376cf7e-e133-4c8a-ab3e-d57137e8ce4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fpdf\n",
      "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fpdf\n",
      "  Building wheel for fpdf (setup.py): started\n",
      "  Building wheel for fpdf (setup.py): finished with status 'done'\n",
      "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40712 sha256=06b2d0adbebd052073d4363ad77f1d496875245268fb64a3e7c80a836f06f0a9\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\65\\4f\\66\\bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
      "Successfully built fpdf\n",
      "Installing collected packages: fpdf\n",
      "Successfully installed fpdf-1.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'fpdf' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'fpdf'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "!pip install fpdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b1b8a5e-ab40-47ca-b152-574f785c4848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ PDF saved to: C:\\Users\\User\\OneDrive\\Desktop\\Ai-Disaster-Alert-System\\Reports\\model_evaluation_report.pdf\n"
     ]
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Create output dir for report ===\n",
    "report_dir = r\"C:\\Users\\User\\OneDrive\\Desktop\\Ai-Disaster-Alert-System\\Reports\"\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "# === Save confusion matrix image ===\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "conf_df = pd.DataFrame(conf_matrix, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "conf_path = os.path.join(report_dir, \"conf_matrix.png\")\n",
    "plt.savefig(conf_path)\n",
    "plt.close()\n",
    "\n",
    "# === Prepare PDF ===\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "\n",
    "# Title\n",
    "pdf.set_font(\"Arial\", \"B\", 16)\n",
    "pdf.cell(200, 10, \"AI Disaster Alert System - Model Evaluation Report\", ln=True, align='C')\n",
    "\n",
    "# Accuracy\n",
    "pdf.ln(10)\n",
    "pdf.set_font(\"Arial\", \"\", 12)\n",
    "pdf.cell(200, 10, f\"Accuracy: {acc * 100:.2f}%\", ln=True)\n",
    "\n",
    "# Classification Report\n",
    "report_text = classification_report(y_true, y_pred)\n",
    "pdf.ln(10)\n",
    "pdf.set_font(\"Arial\", \"B\", 14)\n",
    "pdf.cell(200, 10, \"Classification Report:\", ln=True)\n",
    "pdf.set_font(\"Courier\", \"\", 10)\n",
    "for line in report_text.splitlines():\n",
    "    pdf.cell(200, 6, line.strip().encode(\"latin-1\", \"ignore\").decode(\"latin-1\"), ln=True)\n",
    "\n",
    "# Confusion Matrix Image\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", \"B\", 14)\n",
    "pdf.cell(200, 10, \"Confusion Matrix:\", ln=True)\n",
    "pdf.image(conf_path, x=40, y=30, w=130)\n",
    "\n",
    "# Save PDF\n",
    "pdf_path = os.path.join(report_dir, \"model_evaluation_report.pdf\")\n",
    "pdf.output(pdf_path)\n",
    "\n",
    "print(f\"üìÑ PDF saved to: {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d600e7-8abe-413a-92bb-16955143b9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
